{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import datetime\n",
    "time_format = f'{datetime.now().strftime(\"%y%m%d\")}'\n",
    "print(time_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240508\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "time_format = f'{datetime.now().strftime(\"%y%m%d\")}'\n",
    "print(time_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in TMLR: 861\n"
     ]
    }
   ],
   "source": [
    "\n",
    "journal_pattern = {\n",
    "    'pmlr': ['https://proceedings.mlr.press/v%s/', [1, 240], ['R0', 'R1', 'R2', 'R3', 'R4', 'R5']],\n",
    "    'jmlr': ['https://www.jmlr.org/papers/v%s/', [1, 25]],\n",
    "    'dmlr': ['https://data.mlr.press/volumes/%s.html', [1, 1]],\n",
    "    'tmlr': ['https://jmlr.org/tmlr/papers/'],\n",
    "    'mloss': ['https://www.jmlr.org/mloss/'],\n",
    "}\n",
    "\n",
    "all_papers = {}\n",
    "\n",
    "tmlr_url='https://jmlr.org/tmlr/papers/'\n",
    "data = requests.get(tmlr_url)\n",
    "data.encoding = data.apparent_encoding\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "item_nocertificate = soup.find_all('li', {'class': 'item nocertificate'})\n",
    "print(f\"Number of papers in TMLR: {len(item_nocertificate)}\")\n",
    "tmlr_papers = []\n",
    "for idp, item in enumerate(item_nocertificate):\n",
    "    # print(item)\n",
    "    paper = {}\n",
    "    paper['title'], paper['url'] = item.find('h4').text, item.find('h4').find('a')['href']\n",
    "    paper['author'], paper['time_pub'] = item.find('p').find('i').text, item.find('p').get_text(strip=True).split('[')[0].split(', ')[-1]\n",
    "    hrefs = item.find('p').find_all('a')\n",
    "    for href in hrefs:\n",
    "        if 'bib' in href.text:            \n",
    "            paper[href.text] = f\"https://jmlr.org{href['href']}\"\n",
    "        else:\n",
    "            paper[href.text] = href['href']\n",
    "    # print(idp, paper)\n",
    "    tmlr_papers.append(paper)\n",
    "print(f\"Total number in tmlr: {len(tmlr_papers)}\")\n",
    "all_papers['tmlr'] = tmlr_papers\n",
    "\n",
    "\n",
    "dmlr_papers = []\n",
    "dmlr_pattern = journal_pattern['dmlr']\n",
    "for volume in range(dmlr_pattern[1][0], dmlr_pattern[1][1] + 1):\n",
    "    dmlr_url = f'https://data.mlr.press/volumes/{volume:02d}.html'\n",
    "    data = requests.get(dmlr_url)\n",
    "    data.encoding = data.apparent_encoding\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    post_content = soup.find('div', {'class': 'post-content'})\n",
    "    dl_soup = post_content.find_all('dl')\n",
    "    print(f\"Number of paper in volume {volume} url: {dmlr_url} : {len(dl_soup)}\")\n",
    "\n",
    "    for id_pp, pp_infor in enumerate(dl_soup):\n",
    "        paper = {}\n",
    "        paper['title'], paper['author'] = pp_infor.find('dt').get_text(strip=True), pp_infor.find('dd').get_text(strip=True).replace(',', ', ')\n",
    "        paper['abstract'] = pp_infor.find('details').find('p').text\n",
    "        hrefs = pp_infor.find_all('a')\n",
    "        for href in hrefs:\n",
    "            if '[' not in href.text:\n",
    "                continue\n",
    "            if 'bib' in href.text:            \n",
    "                paper[href.text[1:-1].lower()] = f\"https://data.mlr.press/{href['href']}\"\n",
    "            else:\n",
    "                paper[href.text[1:-1].lower()] = href['href']\n",
    "        dmlr_papers.append(paper)\n",
    "print(f\"Total number in dmlr: {len(dmlr_papers)}\")\n",
    "all_papers['dmlr'] = dmlr_papers\n",
    "\n",
    "\n",
    "jmlr_papers = []\n",
    "jmlr_pattern = journal_pattern['jmlr']\n",
    "for volume in range(jmlr_pattern[1][0], jmlr_pattern[1][1] + 1):\n",
    "    jmlr_url = f'https://www.jmlr.org/papers/v{volume}'\n",
    "    data = requests.get(jmlr_url)\n",
    "    data.encoding = data.apparent_encoding\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    print(f\"Process to url volume: {jmlr_url}\")\n",
    "    if volume < 5:\n",
    "        paper_list = soup.find_all('tr')\n",
    "    else:\n",
    "        paper_list = soup.find_all('dl')\n",
    "        \n",
    "    print(f\"Number of paper in this volume {volume} : {len(paper_list)} \")\n",
    "    for idp, item in enumerate(paper_list):\n",
    "        # print(item)\n",
    "        paper = {}\n",
    "        paper['title'] = item.find('dt').text.split('\\n')[0]\n",
    "        dd_soup = item.find('dd')\n",
    "        paper['author'], paper['time_pub'] = dd_soup.find('b').find('i').text, paper_list[0].find('dd').get_text(strip=True).split('[')[0].split('\\n')[-1]\n",
    "        hrefs = dd_soup.find_all('a')\n",
    "        for href in hrefs:\n",
    "            if 'abs' in href.text:            \n",
    "                paper[href.text[1:-1].lower()] = f\"{jmlr_url}/{href['href']}\"\n",
    "            else:\n",
    "                paper[href.text[1:-1].lower()] = href['href']\n",
    "        # print(idp, paper)\n",
    "        jmlr_papers.append(paper)\n",
    "print(f\"Total number in jmlr: {len(jmlr_papers)}\")\n",
    "all_papers['jmlr'] = jmlr_papers\n",
    "\n",
    "pmlr_papers = []\n",
    "pmlr_pattern = journal_pattern['pmlr']\n",
    "for volume in range(pmlr_pattern[1][0], pmlr_pattern[1][1] + 1):\n",
    "    pmlr_url = f'https://proceedings.mlr.press/v{volume}'\n",
    "    \n",
    "    data = requests.get(pmlr_url)\n",
    "    data.encoding = data.apparent_encoding\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    if 'File not found' in soup.text:\n",
    "        print(f\"*****Skip this url volume: {pmlr_url}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Process to url volume: {pmlr_url}\")\n",
    "        \n",
    "    proceedings_name = soup.find('h2').text\n",
    "    print(f\"The name of volume: {proceedings_name}\")\n",
    "    paper_list = soup.find_all('div', {'class': 'paper'})\n",
    "    print(f\"Number of paper in volume {volume}: {len(paper_list)}\")\n",
    "    \n",
    "    for idp, item in enumerate(paper_list):\n",
    "        paper = {}\n",
    "        \n",
    "        paper['title'] = item.find('p', {'class': 'title'}).text\n",
    "        paper['author'] = item.find('span', {'class': 'authors'}).text.replace('\\xa0', ' ')\n",
    "        paper['info'] = item.find('span', {'class': 'info'}).text\n",
    "        paper['proceedings'] = proceedings_name\n",
    "        hrefs = item.find('p', {'class': 'links'}).find_all('a')\n",
    "\n",
    "        for href in hrefs:\n",
    "            if 'pdf' in href.text.lower():            \n",
    "                paper['pdf'] = href['href']\n",
    "            else:\n",
    "                paper[href.text.lower()] = href['href']\n",
    "        # print(idp, paper)\n",
    "        pmlr_papers.append(paper)\n",
    "        \n",
    "print(f\"Total number in pmlr: {len(pmlr_papers)}\")    \n",
    "all_papers['pmlr'] = pmlr_papers\n",
    "\n",
    "\n",
    "mloss_papers = []\n",
    "data = requests.get('https://www.jmlr.org/mloss/')\n",
    "data.encoding = data.apparent_encoding\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "paper_list = soup.find_all('dl')\n",
    "\n",
    "print(f\"Number of paper in mloss track : {len(paper_list)} \")\n",
    "for idp, item in enumerate(paper_list):\n",
    "    # print(item)\n",
    "    paper = {}\n",
    "    paper['title'] = item.find('dt').text.split('\\n')[0]\n",
    "    dd_soup = item.find('dd')\n",
    "    paper['author'], paper['time_pub'] = dd_soup.find('b').find('i').text, paper_list[0].find('dd').get_text(strip=True).split('[')[0].split('; ')[-1]\n",
    "    hrefs = dd_soup.find_all('a')\n",
    "    for href in hrefs:\n",
    "        if 'abs' in href.text or 'pdf' in href.text or 'bib' in href.text:\n",
    "            paper[href.text] = f\"https://www.jmlr.org{href['href']}\"\n",
    "        else:\n",
    "            paper[href.text] = href['href']\n",
    "\n",
    "all_papers['mloss'] = mloss_papers\n",
    "\n",
    "\n",
    "with open('./tmp/prml_jmlr_dmlr_tmlr_mloss_240508.json', 'w') as fw:\n",
    "    json.dump(all_papers, fw, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./tmp/prml_jmlr_dmlr_tmlr_mloss_240508.json', 'w') as fw:\n",
    "    json.dump(all_papers, fw, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paper in mloss track : 195 \n"
     ]
    }
   ],
   "source": [
    "mloss_papers = []\n",
    "data = requests.get('https://www.jmlr.org/mloss/')\n",
    "data.encoding = data.apparent_encoding\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "paper_list = soup.find_all('dl')\n",
    "\n",
    "print(f\"Number of paper in mloss track : {len(paper_list)} \")\n",
    "for idp, item in enumerate(paper_list):\n",
    "    # print(item)\n",
    "    paper = {}\n",
    "    paper['title'] = item.find('dt').text.split('\\n')[0]\n",
    "    dd_soup = item.find('dd')\n",
    "    paper['author'], paper['time_pub'] = dd_soup.find('b').find('i').text, paper_list[0].find('dd').get_text(strip=True).split('[')[0].split('; ')[-1]\n",
    "    hrefs = dd_soup.find_all('a')\n",
    "    for href in hrefs:\n",
    "        if 'abs' in href.text or 'pdf' in href.text or 'bib' in href.text:\n",
    "            paper[href.text] = f\"https://www.jmlr.org{href['href']}\"\n",
    "        else:\n",
    "            paper[href.text] = href['href']\n",
    "    mloss_papers.append(paper)\n",
    "all_papers['mloss'] = mloss_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paper in this volume 1 : 195 \n",
      "0 {'title': 'QDax: A Library for Quality-Diversity and Population-based Algorithms with Hardware Acceleration', 'author': 'Felix Chalumeau, Bryan Lim, Raphaël Boige, Maxime Allard, Luca Grillotti, Manon Flageat, Valentin Macé, Guillaume Richard, Arthur Flajolet, Thomas Pierrot, Antoine Cully', 'time_pub': '(108):1−16, 2024.', 'abs': 'https://www.jmlr.org/papers/v25/23-1027.html', 'pdf': 'https://www.jmlr.org/papers/volume25/23-1027/23-1027.pdf', 'bib': 'https://www.jmlr.org/papers/v25/23-1027.bib', 'code': 'https://github.com/adaptive-intelligent-robotics/QDax'}\n"
     ]
    }
   ],
   "source": [
    "data = requests.get('https://www.jmlr.org/mloss/')\n",
    "data.encoding = data.apparent_encoding\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "paper_list = soup.find_all('dl')\n",
    "\n",
    "print(f\"Number of paper in this volume {volume} : {len(paper_list)} \")\n",
    "for idp, item in enumerate(paper_list):\n",
    "    # print(item)\n",
    "    paper = {}\n",
    "    paper['title'] = item.find('dt').text.split('\\n')[0]\n",
    "    dd_soup = item.find('dd')\n",
    "    paper['author'], paper['time_pub'] = dd_soup.find('b').find('i').text, paper_list[0].find('dd').get_text(strip=True).split('[')[0].split('; ')[-1]\n",
    "    hrefs = dd_soup.find_all('a')\n",
    "    for href in hrefs:\n",
    "        if 'abs' in href.text or 'pdf' in href.text or 'bib' in href.text:\n",
    "            paper[href.text] = f\"https://www.jmlr.org{href['href']}\"\n",
    "        else:\n",
    "            paper[href.text] = href['href']\n",
    "    # jmlr_papers.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume 1: Gaussian Processes in Practice, 12-13 June 2006, Bletchley Park, UK\n"
     ]
    }
   ],
   "source": [
    "proceedings_name = soup.find('h2').text\n",
    "print(proceedings_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Cedric Archambeau, Dan Cornford, Manfred Opper, John Shawe-Taylor',\n",
       " 'Gaussian Processes in Practice, PMLR 1:1-16')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "item.find('span', {'class': 'authors'}).text.replace('\\xa0', ' '), item.find('span', {'class': 'info'}).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abs': 'https://proceedings.mlr.press/v1/archambeau07a.html', 'pdf': 'http://proceedings.mlr.press/v1/archambeau07a/archambeau07a.pdf'}\n"
     ]
    }
   ],
   "source": [
    "hrefs = item.find('p', {'class': 'links'}).find_all('a')\n",
    "\n",
    "for href in hrefs:\n",
    "    if 'pdf' in href.text.lower():            \n",
    "        paper['pdf'] = href['href']\n",
    "    else:\n",
    "        paper[href.text.lower()] = href['href']\n",
    "print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning with Mixtures of Trees\\nMarina Meila, Michael I. Jordan; \\n1(Oct):1-48, 2000.\\n[abs]\\n[pdf]\\n[ps.gz]\\n[ps]\\n[html]\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.find('dt').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in TMLR: 861\n",
      "Total number in tmlr: 861\n"
     ]
    }
   ],
   "source": [
    "tmlr_url='https://jmlr.org/tmlr/papers/'\n",
    "data = requests.get(tmlr_url)\n",
    "data.encoding = data.apparent_encoding\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "item_nocertificate = soup.find_all('li', {'class': 'item nocertificate'})\n",
    "print(f\"Number of papers in TMLR: {len(item_nocertificate)}\")\n",
    "tmlr_papers = []\n",
    "for idp, item in enumerate(item_nocertificate):\n",
    "    # print(item)\n",
    "    paper = {}\n",
    "    paper['title'], paper['url'] = item.find('h4').text, item.find('h4').find('a')['href']\n",
    "    paper['author'], paper['time_pub'] = item.find('p').find('i').text, item.find('p').get_text(strip=True).split('[')[0].split(', ')[-1]\n",
    "    hrefs = item.find('p').find_all('a')\n",
    "    for href in hrefs:\n",
    "        if 'bib' in href.text:            \n",
    "            paper[href.text] = f\"https://jmlr.org{href['href']}\"\n",
    "        else:\n",
    "            paper[href.text] = href['href']\n",
    "    # print(idp, paper)\n",
    "    tmlr_papers.append(paper)\n",
    "print(f\"Total number in tmlr: {len(tmlr_papers)}\")\n",
    "all_papers['tmlr'] = tmlr_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jmlr_papers = []\n",
    "jmlr_pattern = journal_pattern['jmlr']\n",
    "for volume in range(jmlr_pattern[1][0], jmlr_pattern[1][1] + 1):\n",
    "    jmlr_url = f'https://www.jmlr.org/papers/v{volume}'\n",
    "    data = requests.get(jmlr_url)\n",
    "    data.encoding = data.apparent_encoding\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    print(f\"Process to url volume: {jmlr_url}\")\n",
    "    if volume < 5:\n",
    "        paper_list = soup.find_all('tr')\n",
    "    else:\n",
    "        paper_list = soup.find_all('dl')\n",
    "        \n",
    "    print(f\"Number of paper in this volume {volume} : {len(paper_list)} \")\n",
    "    for idp, item in enumerate(paper_list):\n",
    "        # print(item)\n",
    "        paper = {}\n",
    "        paper['title'] = item.find('dt').text.split('\\n')[0]\n",
    "        dd_soup = item.find('dd')\n",
    "        paper['author'], paper['time_pub'] = dd_soup.find('b').find('i').text, paper_list[0].find('dd').get_text(strip=True).split('[')[0].split('\\n')[-1]\n",
    "        hrefs = dd_soup.find_all('a')\n",
    "        for href in hrefs:\n",
    "            if 'abs' in href.text:            \n",
    "                paper[href.text[1:-1].lower()] = f\"{jmlr_url}/{href['href']}\"\n",
    "            else:\n",
    "                paper[href.text[1:-1].lower()] = href['href']\n",
    "        # print(idp, paper)\n",
    "        jmlr_papers.append(paper)\n",
    "print(f\"Total number in jmlr: {len(jmlr_papers)}\")\n",
    "all_papers['jmlr'] = jmlr_papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meila00a.html'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrefs[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iclr_url = f'https://iclr.cc/virtual/2022/papers.html?filter=titles'\n",
    "data = requests.get(iclr_url)\n",
    "data.encoding = data.apparent_encoding\n",
    "soup = BeautifulSoup(data.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 336\n",
      "2019 501\n",
      "2020 687\n",
      "2021 860\n",
      "2022 1095\n",
      "2023 1584\n",
      "2024 2296\n"
     ]
    }
   ],
   "source": [
    "data_iclr = {}\n",
    "\n",
    "for year in range(2018, 2025, 1):\n",
    "    # papers = soup.find_all('li')\n",
    "    # print(len(papers))\n",
    "    # <li><a href=\"/virtual/2022/poster/6615\">Distributionally Robust Fair Principal Components via Geodesic Descents</a></li>\n",
    "    iclr_url = f'https://iclr.cc/virtual/{year}/papers.html?filter=titles'\n",
    "    data = requests.get(iclr_url)\n",
    "    data.encoding = data.apparent_encoding\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    papers = soup.find_all('li')\n",
    "    cnt = 0\n",
    "    data_year = []\n",
    "    for id, item in enumerate(papers):\n",
    "        \n",
    "        alink = item.find('a')\n",
    "        if f'/virtual/{year}/poster' in alink['href']:\n",
    "            cnt += 1\n",
    "            # print(cnt, alink['href'], alink.text, '/virtual/2022' in alink)\n",
    "            data_year.append({'url': f'https://iclr.cc{alink[\"href\"]}', 'title': alink.text})\n",
    "    print(year, cnt)\n",
    "\n",
    "    if cnt > 0:\n",
    "        data_iclr[f'{year}'] = data_year\n",
    "with open(f'./tmp/iclr_{time_format}.json', 'w') as fw:\n",
    "    json.dump(data_iclr, fw, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import datetime\n",
    "time_format = f'{datetime.now().strftime(\"%y%m%d\")}'\n",
    "print(time_format)\n",
    "\n",
    "data_neurips = {}\n",
    "\n",
    "for year in range(1999, 2025, 1):\n",
    "    neurips_url = f'https://neurips.cc/virtual/{year}/papers.html?filter=titles'\n",
    "    data = requests.get(neurips_url)\n",
    "    data.encoding = data.apparent_encoding\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    papers = soup.find_all('li')\n",
    "    cnt = 0\n",
    "    data_year = []\n",
    "    for id, item in enumerate(papers):\n",
    "        \n",
    "        alink = item.find('a')\n",
    "        if f'/virtual/{year}/poster' in alink['href']:\n",
    "            cnt += 1\n",
    "            # print(cnt, alink['href'], alink.text, '/virtual/2022' in alink)\n",
    "            data_year.append({'url': f'https://neurips.cc{alink[\"href\"]}', 'title': alink.text})\n",
    "    print(year, cnt)\n",
    "\n",
    "    if cnt > 0:\n",
    "        data_neurips[f'{year}'] = data_year\n",
    "# with open(f'./tmp/neurips_{time_format}.json', 'w') as fw:\n",
    "#     json.dump(data_neurips, fw, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import os\n",
    "# import json\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# from datetime import datetime\n",
    "# time_format = f'{datetime.now().strftime(\"%y%m%d\")}'\n",
    "# print(time_format)\n",
    "\n",
    "data_icml = {}\n",
    "\n",
    "for year in range(2017, 2025, 1):\n",
    "    icml_url = f'https://icml.cc/virtual/{year}/papers.html?filter=titles'\n",
    "    data = requests.get(icml_url)\n",
    "    data.encoding = data.apparent_encoding\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    papers = soup.find_all('li')\n",
    "    cnt = 0\n",
    "    data_year = []\n",
    "    for id, item in enumerate(papers):\n",
    "        \n",
    "        alink = item.find('a')\n",
    "        if f'/virtual/{year}/poster' in alink['href']:\n",
    "            cnt += 1\n",
    "            # print(cnt, alink['href'], alink.text, '/virtual/2022' in alink)\n",
    "            data_year.append({'url': f'https://icml.cc{alink[\"href\"]}', 'title': alink.text})\n",
    "    print(year, cnt)\n",
    "\n",
    "    if cnt > 0:\n",
    "        data_icml[f'{year}'] = data_year\n",
    "with open(f'./tmp/icml_{time_format}.json', 'w') as fw:\n",
    "    json.dump(data_icml, fw, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240509\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import datetime\n",
    "time_format = f'{datetime.now().strftime(\"%y%m%d\")}'\n",
    "print(time_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_url = \"https://aclanthology.org/events/emnlp-2023\"\n",
    "data = requests.get(conf_url)\n",
    "data.encoding = data.apparent_encoding\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n",
    "# papers = soup.find_all('li')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2834\n"
     ]
    }
   ],
   "source": [
    "papers = soup.find_all('p', {'class': 'd-sm-flex align-items-stretch'})\n",
    "print(len(papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = soup.find_all('p', {'class': 'd-sm-flex align-items-stretch'})\n",
    "print(len(papers))\n",
    "cnt = 0\n",
    "for id, item in enumerate(papers):  \n",
    "    strong = item.find('strong')\n",
    "    alink = strong.find('a', {'class': 'align-middle'})\n",
    "    # alink = item.find('a', {'class': 'align-middle'})\n",
    "    cnt += 1\n",
    "    print(cnt, alink['href'], alink.text)\n",
    "    paper = {'url': f'https://aclanthology.org/{alink[\"href\"]}', 'title': alink.text}\n",
    "\n",
    "# print(year, cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_url = \"https://aclanthology.org/events/\"\n",
    "data = requests.get(nlp_url)\n",
    "data.encoding = data.apparent_encoding\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1758"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = soup.find_all('li')\n",
    "len(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events: 1754\n"
     ]
    }
   ],
   "source": [
    "nlp_url = \"https://aclanthology.org/events/\"\n",
    "data = requests.get(nlp_url)\n",
    "data.encoding = data.apparent_encoding\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n",
    "events = soup.find_all('li')\n",
    "cnt = 0\n",
    "data_event = []\n",
    "for ide, event in enumerate(events):\n",
    "    alink = event.find('a')\n",
    "    if f'/events/' in alink['href']:\n",
    "        data_event.append({'url': f'https://aclanthology.org{alink[\"href\"]}', 'info': alink.text})\n",
    "print(f'Number of events: {len(data_event)}')\n",
    "\n",
    "for ide, event in enumerate(data_event[:1]):\n",
    "    data = requests.get(event['url'])\n",
    "    data.encoding = data.apparent_encoding\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    papers = soup.find_all('p', {'class': 'd-sm-flex align-items-stretch'})\n",
    "    print(f\"Number of paper in event: {event['info']}: {len(papers)}\")\n",
    "    data_event = []\n",
    "    for id, item in enumerate(papers):  \n",
    "        strong = item.find('strong')\n",
    "        alink = strong.find('a', {'class': 'align-middle'})\n",
    "        paper = {'url': f'https://aclanthology.org/{alink[\"href\"]}', 'title': alink.text, 'info': event['info']}\n",
    "        data_event.append(paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paper in event: 1st Workshop on Open Community-Driven Machine Translation (2023): 6\n"
     ]
    }
   ],
   "source": [
    "for ide, event in enumerate(data_event[:1]):\n",
    "    data = requests.get(event['url'])\n",
    "    data.encoding = data.apparent_encoding\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    papers = soup.find_all('p', {'class': 'd-sm-flex align-items-stretch'})\n",
    "    print(f\"Number of paper in event: {event['info']}: {len(papers)}\")\n",
    "    data_event = []\n",
    "    for id, item in enumerate(papers):  \n",
    "        strong = item.find('strong')\n",
    "        alink = strong.find('a', {'class': 'align-middle'})\n",
    "        paper = {'url': f'https://aclanthology.org/{alink[\"href\"]}', 'title': alink.text, 'info': event['info']}\n",
    "        data_event.append(paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
